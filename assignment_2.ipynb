{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f455882",
   "metadata": {},
   "source": [
    "# ECON 5181 Assignment 2\n",
    "\n",
    "Due: April 5, 2024, noon. Submission on Blackboard.\n",
    "\n",
    "DON'T forget your VeriGuide report and honesty declaration! \n",
    "\n",
    "*The copyright of this content, produced by staff members/ teachers of The Chinese University of Hong Kong (CUHK), belongs to CUHK.  Students shall not distribute, share, copy or upload the content to a third party.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bfb55",
   "metadata": {},
   "source": [
    "## Q1. Regression trees\n",
    "\n",
    "We consider the following data generating process:\n",
    "\n",
    "$$Y_i = X_{i1} + X_{i2}  + U_i$$\n",
    "\n",
    "where $X_1, X_2$ are independent standard normal random variables, and $U$ is standard normal, independent of the $X$'s. \n",
    "\n",
    "In this exercise, we study the behavior of regression trees in this additive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798efbc",
   "metadata": {},
   "source": [
    "#### 1. Simulate $S = 500$ samples according to the DGP, each with a sample size $n = 1600$. In each simulated sample, randomly split the data into 1200 observations that will form a training set and 400 observations that will form the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ed576",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Set model primitives\n",
    "# INPUT your code here\n",
    "\n",
    "Nsim <- 500\n",
    "N <- 1600\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc2312",
   "metadata": {},
   "source": [
    "#### 2. Estimate a regression tree of $Y_i$ on $X_{i1}, X_{i2}$ in each simulated sample. \n",
    "\n",
    "You will use R pacakge to implement CART, which does pruning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265d59b9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>TRUE</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\item TRUE\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. TRUE\n",
       "2. TRUE\n",
       "3. TRUE\n",
       "4. TRUE\n",
       "5. TRUE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] TRUE\n",
       "\n",
       "[[2]]\n",
       "[1] TRUE\n",
       "\n",
       "[[3]]\n",
       "[1] TRUE\n",
       "\n",
       "[[4]]\n",
       "[1] TRUE\n",
       "\n",
       "[[5]]\n",
       "[1] TRUE\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## List the packages we need, install if missing, then load all of them\n",
    "# data.table is an enhanced version of data.frame\n",
    "# ggplot makes beautiful plots and is flexible and powerful, especially with overlaying\n",
    "\n",
    "PackageList =c('tree', 'rpart', 'randomForest', 'data.table', 'ggplot2')\n",
    "NewPackages=PackageList[!(PackageList %in%\n",
    "installed.packages()[,\"Package\"])]\n",
    "\n",
    "if(length(NewPackages)) install.packages(NewPackages)\n",
    "\n",
    "lapply(PackageList,require,character.only=TRUE)#array function\n",
    "\n",
    "set.seed(156) #Always set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8967c57c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'tree'"
      ],
      "text/latex": [
       "'tree'"
      ],
      "text/markdown": [
       "'tree'"
      ],
      "text/plain": [
       "[1] \"tree\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write a function to estimate a regression tree\n",
    "# INPUT your code here\n",
    "\n",
    "\n",
    "# Here is an example that I wrote using rpart, which offers more controls than the tree package \n",
    "# we saw earlier in the lecture codes\n",
    "estimate_tree <- function(formula, train_dt){\n",
    "    big.tree <- rpart(formula = formula, data = train_dt, method = \"anova\", control = rpart.control(minsplit = 5,\n",
    "                                                                                    cp = 1e-4,\n",
    "                                                                                    xval = 10))\n",
    "    bestcp <- big.tree$cptable[which.min(big.tree$cptable[, 'xerror']), \"CP\"]\n",
    "    best.tree <- prune(big.tree, cp = bestcp)\n",
    "\n",
    "    return(best.tree)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e043165c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Create an array to store the tree estimate for all simulations\n",
    "# INPUT your code here\n",
    "\n",
    "rmse <- rep(0, Nsim)\n",
    "leaves <- rep(0, Nsim)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdcb43",
   "metadata": {},
   "source": [
    "#### 3. Pick any simulated sample. Plot the training data points on a plane and use color to signify the values of $y$.  On the same plot, overlay with the prediction surface of your estimated tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59e18c35",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "# scatterplot with ggplot2: color by y values\n",
    "sp <-ggplot(train_dt, aes(x=x1, y=x2, color=y)) + geom_point()\n",
    "\n",
    "# continuous color gradient\n",
    "sp + scale_color_gradient2(midpoint=mid, low=\"blue\", mid=\"white\",\n",
    "                     high=\"red\", space =\"Lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6b673",
   "metadata": {},
   "source": [
    "In regression problems, we aim to come up with a model that yields the best predictive performance (on new data). Empirically, an estimate of the expected error is the mean squared error (MSE), defined as the average squared distance between prediction $\\hat{Y}_i$ and true value $Y_i$ for observations in the test set. In Machine learning papers, people typically report the square root of MSE to evaluate and compare the performances of different regression models. This metric is known as RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6092631",
   "metadata": {},
   "source": [
    "#### 4. Report the mean and standard deviation of *the number of leaves* and the root mean squared error (*RMSE*) across all simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9cfe5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "# compute RMSE on test data\n",
    "rmse <- sqrt(mean((y - predict(best.tree, test_dt))^2))\n",
    "\n",
    "# compute the number of leaves of a given tree\n",
    "leaves_w[s] <- length(unique(best.tree$where))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7b478",
   "metadata": {},
   "source": [
    "#### 5. Repeat the previous steps, but now using a regression tree of $Y_i$ on $W_i = X_{i1} + X_{i2}$. Compare the number of leaves and RMSE obtained in this model with those in the previous model. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0abfda18",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "# add one column W to your data (both train and test)\n",
    "\n",
    "# use the previously defined function to estimate trees\n",
    "\n",
    "# output a table with two columms: number of leaves, RMSE and two rows: tree with X, tree with W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d55bd9b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your comment here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e2db0",
   "metadata": {},
   "source": [
    "#### 6. Repeat the previous steps, now using a 1-hidden layer neural network with $K=2$ hidden units. Compare the results for trees and neural networks when trained on $X$ vs $W$. Discuss the similarities and differences between these two methods. \n",
    "\n",
    "You will use a R package to train a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820c050",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "\n",
    "\n",
    "# Here is an example that I wrote to train a simple neural network.\n",
    "# I use neuralnet, which is more sophisticated than the package nnet\n",
    "# that we saw earlier in the lecture codes\n",
    "nn_x <- neuralnet(y ~ x1 + x2, data = train_dt, hidden = 2, act.fct = \"logistic\", \n",
    "                  linear.output = TRUE, stepmax = 1e6) # regression, set linear.output to TRUE\n",
    "\n",
    "# \"logistic\" is just our sigmoid function, you can verify by defining the sigmoid function explicitly\n",
    "sigmoid = function(x) {\n",
    "  1/(1 + exp(-x))\n",
    "}\n",
    "\n",
    "# output a table with two columms: number of leaves, RMSE and four rows: tree with X, tree with W, \n",
    "# nnet with X, nnet with W\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c8ecd17",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your comment here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce108aa0",
   "metadata": {},
   "source": [
    "## Q2. Logistic regression\n",
    "\n",
    "In class, we mentioned that neural network without the hidden layer will degenerate to a logistic regression model. In this exercise, we explore this frequently used nonlinear model in predicting a binary response $Y_i \\in \\{0, 1\\}$. \n",
    "\n",
    "The model has the form:\n",
    "$$Pr(Y = 1|X=x;\\beta) = \\frac{\\exp (x'\\beta)}{1 + \\exp(x'\\beta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7e495",
   "metadata": {},
   "source": [
    "#### 1. Write down the log-likelihood function $l(b)$ for observing $\\{x_i, y_i\\}, i = 1, \\cdots, n$ in the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c5c473c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb22067a",
   "metadata": {},
   "source": [
    "#### 2. We estimate the model by maximum likelihood. Is the log-likelihood function convex? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ae81cb0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df3970",
   "metadata": {},
   "source": [
    "#### 3. We use gradient descent to solve the optimization problem. Derive the gradient $\\nabla l(b)$. Program a procedure for estimating a logistic regression model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "431db6f4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78f17069",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here by filling in the missing code below\n",
    "\n",
    "# objective function\n",
    "obj_function = function(X, y, b){\n",
    "    \n",
    "}\n",
    "# gradient function\n",
    "grad_function = function(X, y, b){\n",
    "    \n",
    "}\n",
    "# function to do gradient descent\n",
    "grad_descent <- function(l, grad_l, X, y, alpha, max_iter, tol){\n",
    "    \n",
    "}\n",
    "\n",
    "# Run gradient descent to estimate the model parameters\n",
    "logistic_gr <- grad_descent(l = obj_function, grad_l = grad_function, X, y, alpha, max_iter, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31e4dd",
   "metadata": {},
   "source": [
    "Credit scoring is a classic problem in classification. When a bank receives a loan application, based on the applicant’s profile the bank has to make a decision regarding whether to go ahead with the loan approval or not. Such decisions ideally should rely on an assessment of default risk.\n",
    "\n",
    "We turn to the widely used German credit data to investigate binary classification using logistic regression model and others. Data and code are on Blackboard in `credit.zip`. \n",
    "\n",
    "The dataset consists of 1000 records of approved loan applications. The dependent variable for your purpose is `default`, which codes whether the credit contract has been complied with (0) or not (1). Other variables in the dataset are borrower and loan characteristics, including a person's prior credit history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e19e1",
   "metadata": {},
   "source": [
    "Data is downloaded from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). Some data cleaning needs to be done before we can comfortably work with it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f8481",
   "metadata": {},
   "source": [
    "Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a2d4e8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## naref: make missing (NA) the reference level of a factor\n",
    "\n",
    "xnaref <- function(x){\n",
    "\tif(is.factor(x))\n",
    "\t\tif(!is.na(levels(x)[1]))\n",
    "\t\t\tx <- factor(x,levels=c(NA,levels(x)),exclude=NULL)\n",
    "\treturn(x) }\n",
    "\n",
    "naref <- function(DF){\n",
    "\tif(is.null(dim(DF))) return(xnaref(DF))\n",
    "\tif(!is.data.frame(DF)) \n",
    "\t\tstop(\"You need to give me a data.frame or a factor\")\n",
    "\tDF <- lapply(DF, xnaref)\n",
    "\treturn(as.data.frame(DF))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7eb3b",
   "metadata": {},
   "source": [
    "Read data and create some interesting variables. Helpful to look at the codebook and figure out the meaning of original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ae3771",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "credit <- read.csv(\"./credit/credit.csv\")\n",
    "\n",
    "## re-level the credit history and checking account status\n",
    "credit$history = factor(credit$history, levels=c(\"A30\",\"A31\",\"A32\",\"A33\",\"A34\"))\n",
    "\n",
    "levels(credit$history) = c(\"good\",\"good\",\"poor\",\"poor\",\"terrible\")\n",
    "\n",
    "## a few others\n",
    "credit$foreign <- factor(credit$foreign, levels=c(\"A201\",\"A202\"), labels=c(\"foreign\",\"german\"))\n",
    "credit$rent <- factor(credit$housing==\"A151\")\n",
    "credit$purpose <- factor(credit$purpose, levels=c(\"A40\",\"A41\",\"A42\",\"A43\",\"A44\",\"A45\",\"A46\",\"A47\",\"A48\",\"A49\",\"A410\"))\n",
    "levels(credit$purpose) <- c(\"newcar\",\"usedcar\",rep(\"goods/repair\",4),\"edu\",NA,\"edu\",\"biz\",\"biz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de182e2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1000</li>\n",
       "\t<li>22</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1000\n",
       "\\item 22\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1000\n",
       "2. 22\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1000   22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3861d511",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot a couple of dimensions\n",
    "par(mfrow=c(1,2))\n",
    "\n",
    "plot(factor(Default) ~ history, data=credit, col=c(8,2), ylab=\"Default\") ## surprise!\n",
    "plot(factor(Default) ~ purpose, data=credit, col=c(8,2), ylab=\"\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ec116",
   "metadata": {},
   "source": [
    "This looks counterintuitive -- borrowers with great prior credit history default more frequently! What could be the story here? Or is it due to mistakes in recording the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549338b2",
   "metadata": {},
   "source": [
    "#### 4. Choose a reasonable step-size $\\alpha$. Plot the value of the objective function on each iteration of gradient descent, with the iteration number on the horizontal axis and the objective value\ton the vertical axis. \n",
    "\n",
    "Make sure to include axis labels and a title for your plot. Draw the plot described above for three cases: too small $\\alpha$, reasonable $\\alpha$, and too big $\\alpha$.  You should be able to see the objective function decreases quickly and then becomes stable if the scale of $\\alpha$ is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efaa12cc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab108aa",
   "metadata": {},
   "source": [
    "#### 5. Use the  reasonable $\\alpha$ you get and run the gradient descent on the German credit data. Report the number of iterations that are required for the algorithm to converge. Report the estimated regression coefficients. Also, implement logistic regression using R package `glm`. Does your programmed procedure agree with the canned routine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c6cd2df",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# First some data cleaning and prep\n",
    "credit <- naref(credit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b576e3d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "## logistic regression with your program\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a37803bc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "## logistic regression with glm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Here is one regression specification that I wrote\n",
    "# Sparse design matrix for efficient computation\n",
    "library(gamlr)\n",
    "credx <- sparse.model.matrix( ~ duration + amount +\n",
    "    installment + age + history +\n",
    "    purpose + foreign + rent, data=credit)\n",
    "\n",
    "default <- credit$Default\n",
    "credscore <- glm.fit(credx, default, family = binomial())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1888933",
   "metadata": {},
   "source": [
    "#### 6. There are lots of regressors in the data set, making it hard to interpret the fitted model. We use LASSO penalty to do variable selection. Write down the objective function for the log-likelihood with a $L_1$ penalty, $\\lambda$ being the penalty parameter as usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b535f367",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6d115",
   "metadata": {},
   "source": [
    "#### 7. Use `glmnet` or `gamlr` to estimate the LASSO logistic regression. Report the selected $\\lambda$ using cross validation. How many variables are selected by LASSO under CV, AICc, BIC, and AIC criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa9f5203",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "14"
      ],
      "text/latex": [
       "14"
      ],
      "text/markdown": [
       "14"
      ],
      "text/plain": [
       "[1] 14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "\n",
    "# LASSO logistic regression\n",
    "credscore <- cv.gamlr(credx, default, family=\"binomial\")\n",
    "sum(coef(credscore)!=0) # number of variables selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44f072",
   "metadata": {},
   "source": [
    "#### 8. In your optimal model chosen by cross validation, which variables in $X_i$ are relevant predictors of credit default? Are they all stastistically significant? Choose one significant predictor and interpret the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae8455fd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf6938",
   "metadata": {},
   "source": [
    "In classification problems, the metrics to evaluate predictive accuracy are various: **confusion matrix, accuracy/error/misclassification rate, specificity and sensitivity, and ROC/AUC**. \n",
    "\n",
    "A confusion matrix summarizes all four different scenarios of an actual-predicted pair:\n",
    "\n",
    "|          | Predicted 1     | Predicted 0   |\n",
    "|---------:|:----------------|:--------------|\n",
    "| **Actual 1** | True Positive   | False Negative| \n",
    "| **Actual 0** | False Positive  | True Negative | \n",
    "\n",
    "There are two ways to be wrong in a binary problem.\n",
    "- false positive (FP): predict $\\hat{Y_i} = 1$ when $Y_i=0$\n",
    "- false negative (FN): predict $\\hat{Y_i} = 0$ when $Y_i=1$\n",
    "\n",
    "Logistic regression gives us an estimate of $Pr(Y=1|X=x, \\beta)$. Under Bayes decision rule, classify as 1 (defauter) if the predicted probability $\\hat{p} > 0.5$. However, sometimes making the optimal decision (e.g., granting a loan or not) requires taking into account the cost and benefit of an action. Under certain cost configuration, a profit maximizing bank should lend whenever the probability of default is less than 0.2 (so that the expected profit from lending exceeds that of not lending).\n",
    "\n",
    "For any given cutoff $p$, we can define the following two metrics:\n",
    "- sensitivity: proportion of true $Y=1$ classified as such\n",
    "- specificity: proportion of true $Y=0$ classified as such\n",
    "\n",
    "One index that captures predictive accuracy is *accuracy rate*, which is the sum of TP and TN divided by sample size. Error rate or misclassification rate is $1 - accuracy$. Another commonly used evaluation tool is the ROC curve. It avoids the drawbacks of accuracy rate when classes are highly imbalanced. See the figure below for an example ROC curve. It traces the change in sensitivity and specificity as the cutoff value $p$ goes from 0 to 1. One predictive model is better than the other if the ROC curve lies above it everywhere. Intuitively, for two models that achieve the same level of specificity, the one that yields a higher sensitivity is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "273812a3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>default</th><th scope=col>pred</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0        </td><td>0.1262547</td></tr>\n",
       "\t<tr><td>1        </td><td>0.4622522</td></tr>\n",
       "\t<tr><td>0        </td><td>0.1550758</td></tr>\n",
       "\t<tr><td>0        </td><td>0.4094682</td></tr>\n",
       "\t<tr><td>1        </td><td>0.3792323</td></tr>\n",
       "\t<tr><td>0        </td><td>0.4171023</td></tr>\n",
       "\t<tr><td>0        </td><td>0.2849803</td></tr>\n",
       "\t<tr><td>0        </td><td>0.3340328</td></tr>\n",
       "\t<tr><td>0        </td><td>0.2095200</td></tr>\n",
       "\t<tr><td>1        </td><td>0.3404866</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " default & pred\\\\\n",
       "\\hline\n",
       "\t 0         & 0.1262547\\\\\n",
       "\t 1         & 0.4622522\\\\\n",
       "\t 0         & 0.1550758\\\\\n",
       "\t 0         & 0.4094682\\\\\n",
       "\t 1         & 0.3792323\\\\\n",
       "\t 0         & 0.4171023\\\\\n",
       "\t 0         & 0.2849803\\\\\n",
       "\t 0         & 0.3340328\\\\\n",
       "\t 0         & 0.2095200\\\\\n",
       "\t 1         & 0.3404866\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| default | pred |\n",
       "|---|---|\n",
       "| 0         | 0.1262547 |\n",
       "| 1         | 0.4622522 |\n",
       "| 0         | 0.1550758 |\n",
       "| 0         | 0.4094682 |\n",
       "| 1         | 0.3792323 |\n",
       "| 0         | 0.4171023 |\n",
       "| 0         | 0.2849803 |\n",
       "| 0         | 0.3340328 |\n",
       "| 0         | 0.2095200 |\n",
       "| 1         | 0.3404866 |\n",
       "\n"
      ],
      "text/plain": [
       "   default pred     \n",
       "1  0       0.1262547\n",
       "2  1       0.4622522\n",
       "3  0       0.1550758\n",
       "4  0       0.4094682\n",
       "5  1       0.3792323\n",
       "6  0       0.4171023\n",
       "7  0       0.2849803\n",
       "8  0       0.3340328\n",
       "9  0       0.2095200\n",
       "10 1       0.3404866"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## prediction from our fitted logistic regression model\n",
    "pred <- predict(credscore, credx, type=\"response\")\n",
    "\n",
    "pred <- drop(pred) # remove the sparse Matrix formatting\n",
    "\n",
    "data.frame(default,pred)[1:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43ef50",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## misclassification rates\n",
    "rule <- 1/5 # move this around to see how these change\n",
    "\n",
    "sum( (pred>rule)[default==0] )/sum(pred>rule) ## false positive rate\n",
    "sum( (pred<rule)[default==1] )/sum(pred<rule) ## false negative rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9db7a015",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## ROC curve\n",
    "## function to plot the ROC curve for classification of y with p\n",
    "roc <- function(p,y, ...){\n",
    "  y <- factor(y)\n",
    "  n <- length(p)\n",
    "  p <- as.vector(p)\n",
    "  Q <- p > matrix(rep(seq(0,1,length=100),n),ncol=100,byrow=TRUE)\n",
    "  specificity <- colMeans(!Q[y==levels(y)[1],])\n",
    "  sensitivity <- colMeans(Q[y==levels(y)[2],])\n",
    "  plot(1-specificity, sensitivity, type=\"l\", ...)\n",
    "  abline(a=0,b=1,lty=2,col=8) # what we will get by classifying randomly\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "348d42af",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc(p=pred, y=default, bty=\"n\")\n",
    "\n",
    "## our 1/5 rule cutoff \n",
    "rule <- 1/5 # move this around to see how things change\n",
    "sensitivity<- sum( (pred>rule)[default==1] )/sum(default==1) ## sensitivity\n",
    "specificity<-sum( (pred<rule)[default==0] )/sum(default==0) ## specificity\n",
    "\n",
    "points(x= 1-specificity, \n",
    "\ty=sensitivity, \n",
    "\tcex=1.5, pch=20, col='red') \n",
    "## a standard `max prob' (p=.5) rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdeec32",
   "metadata": {},
   "source": [
    "#### 9. Now, experiment with a few other algorithms -- random forest and neural network -- to predict credit default. Examine the confusion matrix and accuracy rates. Report the test error (i.e., misclassification rate) for all models, including logistic regression and regularized logistic regression, in a summary table. Plot the ROC curve for all models on one single graph. Which algorithm wins in predicting default?\n",
    "\n",
    "You need to split your sample into train (80%) and test (20%) for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2068b1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "# build model and make prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0fc78",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "# create summary table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09660b64",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your code here\n",
    "\n",
    "# create ROC figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1399d9d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc584e1",
   "metadata": {},
   "source": [
    "### Congrats, you are done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a871c",
   "metadata": {},
   "source": [
    "Before you quit the document, here is one thing to ponder. Do loan officers actually make their decision based on these prediction algorithms? If not, how do they fare compared to the algorithms we trained using retrospective data?\n",
    "\n",
    "These turn out to be a difficult and deep questions (let alone the ethical and philosophical issues), but also extremely worthwhile to think about in the age of automation. A few economists teamed up with computer scientists and provided us with some interesting food for thought: [Human Decisions and Machine Predictions in the context of criminal justice](https://doi.org/10.1093/qje/qjx032)\n",
    "\n",
    "Reference: Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan, *Human Decisions and Machine Predictions*, The Quarterly Journal of Economics, Volume 133, Issue 1, February 2018, Pages 237–293"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
